---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message=FALSE,
  warning=FALSE
)
```

# scottish-cycle-counts

<!-- badges: start -->

<!-- badges: end -->

The goal of scottish-cycle-counts is to read-in a process data on cycling volumes in Scotland.

```{r}
library(tidyverse)
```

First of all, a copy of the original files can be obtained with the code below. This code is to be run just once.
```{r, eval=FALSE}
dir.create("data_raw")
system("gh release download 1 --dir data_raw")
```
The input dataset is a single .zip file:

```{r,eval=FALSE}
zipped_data = list.files(path = "data_raw",pattern = "\\.zip$",full.names = T)
zipped_data
```

```{r, echo=FALSE, eval=FALSE}
tail(zipped_data, 1)
```

We can unzip it as follows:
```{r, eval=FALSE}
unzip(zipped_data, exdir = "data_raw")
```



```{r}
files_csv = list.files("data_raw", pattern = "\\.csv$", full.names = TRUE)
files_csv
```

We can read this file in R as follows:

```{r}
# library(data.table)
# counts = data.frame(data.table::rbindlist(lapply(files_csv,data.table::fread))) #DT's quick way to read the files 
counts = map_dfr(files_csv, read_csv,show_col_types = FALSE)
dim(counts)
counts
```

```{r}
counts_monthly = counts |>
  mutate(
    year = year(endTime),
    month = month(endTime, label = TRUE),
    # date rounded to nearest month in 2020-01-01 format:
    date = lubridate::floor_date(endTime, unit = "month")
  ) |>
  group_by(date, area) |>
  summarise(
    count = sum(count)
  )
# Add column with names for most common areas:
# Most common areas are:
area_counts = counts_monthly |>
  group_by(area) |>
  summarise(
    count = sum(count)
  )

top_5_areas = area_counts |> 
  slice_max(count, n = 5)

# Add column that is area name if in top 5, else "Other":
counts_monthly_top = counts_monthly |>
  mutate(
    Area = ifelse(
      area %in% top_5_areas$area,
      area,
      "Other"
    )
  ) |>
  group_by(date, Area) |>
  summarise(
    count = sum(count)
  )
```

```{r}
counts_monthly_top |>
  ggplot(aes(x = date, y = count, colour = Area)) +
  geom_line() +
  # Add log y-axis:
  scale_y_log10()
```

A quick look at the data to check if there are sites with missing data or duplicated records:

```{r}
counts |>
  group_by(siteID,location) |> 
  unique() |> 
  count() |> 
  ggplot(aes(n))+geom_histogram(bins = 30)
```

```{r}
range(counts$startTime)
difftime(range(counts$startTime)[2],range(counts$startTime)[1],units = "days")
```

Each site should have a maximum of 378 days in the dataset. The following code detects the sites with some type of duplication and the ones with fewer records.

```{r}
repeated_sites = counts |>
  group_by(siteID) |> 
  filter(n()>378) |> 
  select(siteID) |>
  unique()

repeated_sites$siteID  |> length()

fewer_sites = counts |>
  group_by(siteID) |> 
  filter(n()<300) |> 
  select(siteID) |>
  unique()

fewer_sites$siteID  |> length()

```

A subset of the clean sites is produced, so we can do some AADT analysis. Records after 2023-06-01 are filtered out to have only one year of data for each site

```{r}

clean_counts = counts |>
  filter(startTime < as.Date("2023-06-01")) |> 
  anti_join(repeated_sites,by =join_by(siteID)) |>
  anti_join(fewer_sites,by =join_by(siteID)) |> 
  filter(n()==365,.by = siteID)

clean_counts |>
  group_by(siteID,location) |> 
  unique() |> 
  count() |> summary()

```

Here we calculate some statistics for the whole year including mean (Average Annual Daily Flow), median daily flow, minimum and maximum daily flows,

```{r}
AADF_sites = clean_counts |> 
  summarise(across(count,list(mean = mean,
                              median = median,
                              min = min,
                              max = max)),.by = siteID)

AADF_sites
```

```{r}
counts_per_area = counts |> select(siteID,area) |> unique()

AADF_sites |>
  left_join(counts_per_area,by = join_by(siteID)) |> 
  mutate(
    area = ifelse(
      area %in% top_5_areas$area,
      area,
      "Other"
    )) |> 
  ggplot(aes(x=fct_reorder(area,count_mean,.desc = T),
             y=count_mean))+
  geom_boxplot(outlier.shape = NA)+
  geom_jitter(aes(col = area),alpha = 0.2,shape = 19,show.legend = F)+
  coord_cartesian(ylim = c(0,1000))+
  theme_minimal()+
  labs(x = "Area",
       y = "AADF")
```

We create a vector to store the bank holidays in Scotland extracted from the [mygov.scot web](https://www.mygov.scot/scotland-bank-holidays)

```{r}
scot_bank_holidays = as.Date(c("2022/06/03",
                               "2022/06/02",
                               "2022/08/1",
                               "2022/11/30",
                               "2022/12/25",
                               "2022/12/26",
                               "2022/12/27",
                               "2023/01/02",
                               "2023/01/03",
                               "2023/04/07",
                               "2023/05/01",
                               "2023/05/08",
                               "2023/05/29"))
```

We can calculate the same summary statistics by type of day: bank holidays, weekends and weekdays (AAWDF).

```{r}
ADF_dtype = clean_counts |> 
  mutate(d.type = case_when(as.Date(startTime) %in% scot_bank_holidays~"Bank Holiday",
                            wday(startTime,week_start = 1)<6~"Weekday",
                            TRUE~"Weekend")) |> 
  summarise(across(count,list(mean = mean,
                              median = median,
                              min = min,
                              max = max)),
            .by = c(siteID,d.type))

ADF_dtype
```

```{r}
counts_per_area = counts |> select(siteID,area) |> unique()

ADF_dtype |>
  left_join(counts_per_area,by = join_by(siteID)) |> 
  mutate(
    area = ifelse(
      area %in% top_5_areas$area,
      area,
      "Other"
    )) |> 
  ggplot(aes(x=fct_reorder(area,count_mean,.desc = T),
             y=count_mean))+
  geom_boxplot(outlier.shape = NA)+
  geom_jitter(aes(col = area),alpha = 0.2,shape = 19,show.legend = F)+
  coord_cartesian(ylim = c(0,1000))+
  facet_wrap(d.type~.,ncol = 2)+
  theme_minimal()+
  labs(x = "Area",
       y = "ADF")


```

## Spatial Analysis

```{r}
library(sf)
library(tmap)
```
The following code reads the network with the estimated commute trips from the npt repository

```{r}
rnet_commute = read_rds("../npt/outputs/rnet_commute.Rds")
rnet_commute
```
A `sf` object is created from the `clean_counts` data frame.

```{r}
sf_counts = clean_counts |>
  select(siteID,latitude,longitude,provider,location) |>
  unique() |>
  st_as_sf(coords = c("longitude","latitude"),crs = 4326)
sf_counts
```



A subset of the counts are taken based on a buffer of the `rnet_commute` object.

```{r}
rnet_buffer20 = rnet_commute |> st_buffer(dist = 20)

sf_counts_selected = sf_counts[rnet_buffer20,]

```

### Approach A
The nearest feature is joined to each point location
```{r}
sf_counts_joined = st_join(sf_counts_selected,rnet_commute,join = st_nearest_feature)
sf_counts_joined
```

```{r}
val_app1 = sf_counts_joined |> left_join(AADF_sites,by = "siteID") |> filter(count_mean > 0)
```

```{r}
tm_shape(rnet_commute)+
  tm_lines(col = "bicycle",lwd = "bicycle")+
  tm_shape(val_app1)+
  tm_dots(col = "count_mean")
```


```{r}
val_app1 |> 
  st_drop_geometry() |>
  mutate(ratio = bicycle/count_mean) |> 
  ggplot(aes(ratio))+
  geom_histogram()
```
```{r}
val_app1 |>
  st_drop_geometry() |>
  ggplot(aes(x = count_mean,
             y = bicycle))+
  geom_point()+
  geom_smooth(method = "lm",
              formula = 'y ~ x',
              se = F)
```

```{r, echo=FALSE}
# Convert README.Rmd to counts.R:  
knitr::purl("README.Rmd", "counts.R")
```